1] Type of Recommendation Systems

1) Contend based - (based on content similarities)On the bases of similarities of contents (similar Tags)
2) Collaborative Filtering - (based of user similarities)(Eg FaceBook) Eg. there are two person A and B , both watch an certain movie and they give rating to that movie , by the raiting we can say that A & B have similar taste for the movies and if A watch another movie he /she like it then system will recommand that movie to B. 
3) Hybrid - 


2] Project Flow

DATA -----> Preprocessing ----> Model ----> Website ----> Deploy
				(1) 			 (2) 		  (3) 			(4) 
				


1) Read it and put it in variable using read_csv

2) merge movies and credits csv file  

2) Remove the columns that is not necessary criteria to recommand a movie to the user
	
	Example 
	1) Budget column - not necessary
	2) genres column - necessary
	3) Homepage column - not necessary
	etc
	
	
	

After Preprocessing

Data format --> MovieID | Movie Name | Tags

1) Text Vectorization

It Technique
1) Bag of Words			<-- we will be using this technique
2) Tfidf
3) Word2Vect

2) Remove StopWords

Eg words like 'is', 'are', 'or', 'the' this words are stopwords

large text contain many stopwords which may give wrong results
StopWords are those words who does not have any meaning but contribute in the sentence formation.

you can Remove StopWords by 

	1) Manually 
	2) Library(sklearn)  


3) Apply steming

	Now there may contain words like( action, actions ) which are same but act as different feature, 
	so to remove duplicate meaning words, we apply Steming.
	
	use nltk library
	

Bag of Words

1) Combine all the tags to large text
	tag1 + tag2 + tag3 ...... = large_text
	
	Large_text contain more that 10,000 words
	Calculate frequency of all the words
	Next 
	
2) For example we take 5000 most common words(high frequency words) from large_text 
	
	W1 = Word 1 - (eg Action)
	M1 = Movie 1 - (eg Avtar)
	
	[M1,W1] = 5 (M1 contain W1 5 times)
	
	shape = (no of movies, most common words) = (5000,5000)
	
			W1(Action) | W2(Adventure) | W3(Romance) | W4 | ............ | W5000
			
	M1		 5				3				2			0					1
	
	M2		 2				0				1			3					0
	
	M3		 5				7				7			3					9
	
	M4
	
	.
	.
	.
	.
	.
	.
	
	M5000

	
3) Let say the shape of data is (5000,2)
	
	
			W1(Action) | W2(Adventure) 
			
	M1		 5				3			
	
	M2		 2				0			
	
	M3		 5				7			
	
	M4
	
	.
	.
	.
	.
	.
	.
	
	M5000
	
	
	
	
	Graph will be like
	
	
	Adventure	|
				|		O M1
				|	   /
				|	  /		O M2
				|    /	  /
				|  /    /
				|/___/_______________
									Action
									
	
	If we like M1 then we will find most similar(closest) movie (eg 5 most closest move) to recommand
									
	This is for 2d (W1(Action), W2(Adventure) )  What if 5000d (W1(Action) , W2(Adventure) , W3(Romance) , W4 , ............ , W5000)
	

Question -  Why we take 5000 words 
Ans - It is totally depend on us, the more words it has more complex it get,  



4) Find nearest Vector
	
   Now we have 4806 movies(Vectors) & each movie has 5000 words(features)
   
   we can't find eucleaden distance between 2 vector instead we will calculate cosine distance
   
   the more high dimension data is the more eucleaden distance fails
   
   distance is inversely proportional to similarity
   
   dist - proportinal - (1/similarity)
